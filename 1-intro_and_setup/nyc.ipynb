{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöñ Advanced NYC Taxi Duration Prediction: Production-Ready ML\n",
    "\n",
    "## Welcome to the Next Level!\n",
    "\n",
    "This notebook builds upon our previous regression project for NYC Taxi Trip Duration prediction. Here, we move from **basic model experiments** to **production-grade workflows**. You'll learn how to:\n",
    "\n",
    "1. Build **scikit-learn pipelines** to streamline preprocessing and modeling.\n",
    "2. Integrate **MLflow** for experiment tracking and reproducibility.\n",
    "3. Train **advanced regression models** beyond Linear, Ridge, and Lasso.\n",
    "4. Perform **cross-validation** and **hyperparameter tuning** for optimal performance.\n",
    "5. Conduct **robust evaluation and comparison** using multiple metrics.\n",
    "6. Prepare for deployment with **model versioning and packaging**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Goals\n",
    "\n",
    "* Understand **modern ML workflows** used in industry.\n",
    "* Build **scalable and maintainable pipelines** for real-world data.\n",
    "* Apply **best practices** for model evaluation and reproducibility.\n",
    "* Transition your ML experiments from **notebook prototypes** to **production-ready systems**.\n",
    "\n",
    "> ‚ö° **Pro Tip:** Keep your code modular and well-documented‚Äîthis is key to production readiness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Production Environment Setup\n",
    "\n",
    "Before we dive into building **advanced pipelines, models, and tracking experiments**, we need to establish a **robust production-ready environment**. This ensures that:\n",
    "\n",
    "1. **Experiments are reproducible**  \n",
    "   - Fixed random seeds, consistent preprocessing, and standardized metrics ensure that results can be trusted and reproduced later.\n",
    "\n",
    "2. **Our workflow is organized and maintainable**  \n",
    "   - Proper imports, library versions, and modular code make it easy for teams to collaborate.\n",
    "\n",
    "3. **Models can be tracked and versioned**  \n",
    "   - Using **MLflow**, we log experiments, hyperparameters, and metrics for easy comparison and auditability.\n",
    "\n",
    "4. **Scalability and flexibility**  \n",
    "   - Preconfigured pipelines, preprocessing tools, and ML libraries allow us to handle larger datasets or swap models seamlessly.\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Pro Tip:** Think of this step as setting the foundation of a building ‚Äî if it's strong and well-organized, everything built on top will be reliable, scalable, and easier to maintain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Advanced imports for production ML\n",
    "\n",
    "# Suppress warnings for cleaner outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üîß Core Python libraries\n",
    "import numpy as np           # Efficient numerical computations\n",
    "import pandas as pd          # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Basic plotting\n",
    "import seaborn as sns        # Advanced visualization\n",
    "from scipy import stats      # Statistical functions\n",
    "import joblib               # Save/load large models and preprocessing objects\n",
    "import json                 # Handle JSON configs and outputs\n",
    "from datetime import datetime  # Timestamping for logs\n",
    "import os                   # File system operations\n",
    "import time                 # Time tracking for experiments\n",
    "\n",
    "# üß∞ Sklearn libraries - expanded for advanced ML workflows\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,     # Split data into train/test sets\n",
    "    cross_val_score,      # Cross-validation scoring\n",
    "    GridSearchCV,         # Hyperparameter tuning (grid search)\n",
    "    RandomizedSearchCV    # Hyperparameter tuning (randomized search)\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,       # Feature scaling (zero-mean, unit variance)\n",
    "    RobustScaler,         # Scaling robust to outliers\n",
    "    PolynomialFeatures    # Generate polynomial features for non-linear relationships\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion  # Build modular pipelines\n",
    "from sklearn.compose import ColumnTransformer         # Apply different preprocessing to columns\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,          # Univariate feature selection\n",
    "    f_regression,         # Scoring function for regression\n",
    "    RFE                   # Recursive feature elimination\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,     # Baseline regression\n",
    "    Ridge,                # L2-regularized regression\n",
    "    Lasso,                # L1-regularized regression\n",
    "    ElasticNet            # Combination of L1 and L2 regularization\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,       # Ensemble of decision trees\n",
    "    GradientBoostingRegressor,   # Boosted trees for regression\n",
    "    VotingRegressor              # Combine multiple regressors\n",
    ")\n",
    "from sklearn.svm import SVR               # Support Vector Regression\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,  # Regression metric\n",
    "    r2_score,            # Regression metric\n",
    "    mean_absolute_error  # Regression metric\n",
    ")\n",
    "from sklearn.inspection import (\n",
    "    permutation_importance,       # Feature importance\n",
    "    PartialDependenceDisplay      # Partial dependence plots\n",
    ")\n",
    "\n",
    "# üß™ Advanced model tracking with MLflow\n",
    "import mlflow                  # Experiment tracking\n",
    "import mlflow.sklearn          # Log sklearn models\n",
    "from mlflow.models.signature import infer_signature  # Auto-capture input/output schema for reproducible deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Configuration & Reproducibility\n",
    "\n",
    "Before we start modeling, it's crucial to **establish reproducible and scalable experiment settings**. This ensures that results are consistent, experiments are traceable, and your workflow is production-ready.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Reproducibility**\n",
    "   - `RANDOM_STATE = 42`: Ensures that every run produces the same train/test splits, random sampling, and model results.\n",
    "   - `TEST_SIZE = 0.2`: Reserves 20% of the data for final evaluation.\n",
    "   - `VAL_SIZE = 0.2`: Reserves a portion of the training data for validation and hyperparameter tuning.\n",
    "   - `CV_FOLDS = 5`: Use 5-fold cross-validation to evaluate models robustly.\n",
    "   - `N_JOBS = -1`: Utilizes all available CPU cores to speed up computations.\n",
    "\n",
    "2. **Organized Project Structure**\n",
    "   - `MODEL_DIR = \"models\"`: All trained models are stored in a dedicated folder.\n",
    "   - `EXPERIMENT_DIR = \"experiments\"`: All MLflow experiment logs, metrics, and artifacts are saved in a centralized location.\n",
    "   - `os.makedirs(..., exist_ok=True)`: Ensures directories exist, preventing file errors during training or logging.\n",
    "\n",
    "3. **MLflow Experiment Tracking**\n",
    "   - `mlflow.set_tracking_uri(...)` points MLflow to the experiment folder.\n",
    "   - `mlflow.set_experiment(experiment_name)` creates a named experiment for this project.\n",
    "   - This allows you to **log models, metrics, and hyperparameters**, enabling easy comparison across experiments.\n",
    "\n",
    "> üí° **Pro Tip:** Proper configuration and tracking is like laying the foundation of a building‚Äîeverything built on top will be reliable, reproducible, and easy to maintain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///workspaces/mlops-zoomcamp/1-intro_and_setup/experiments_nyc_taxi/939151119660141953', creation_time=1770039600165, experiment_id='939151119660141953', last_update_time=1770039600165, lifecycle_stage='active', name='nyc_taxi_duration_advanced', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    # Reproducibility - Critical for production!\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.2  # NEW: Validation set for tuning\n",
    "    CV_FOLDS = 5\n",
    "    N_JOBS = -1  # Use all available cores\n",
    "    \n",
    "    # Model directories - Organized project structure\n",
    "    MODEL_DIR = \"models_nyc_taxi\"\n",
    "    EXPERIMENT_DIR = \"experiments_nyc_taxi\"\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Initialize MLflow for experiment tracking\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(config.EXPERIMENT_DIR)}\")\n",
    "experiment_name = \"nyc_taxi_duration_advanced\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Loading and Preparing NYC Taxi Dataset\n",
    "\n",
    "We'll use a sample of the NYC Taxi Trip Duration dataset. This dataset contains information about taxi trips in New York City, with the goal of predicting trip duration.\n",
    "\n",
    "**Dataset Features:**\n",
    "- `pickup_longitude`, `pickup_latitude`: Pickup location coordinates\n",
    "- `dropoff_longitude`, `dropoff_latitude`: Dropoff location coordinates\n",
    "- `passenger_count`: Number of passengers\n",
    "- `distance`: Trip distance in miles (calculated from coordinates)\n",
    "- `pickup_hour`, `pickup_dayofweek`: Temporal features\n",
    "- `vendor_id`: Taxi vendor identifier\n",
    "\n",
    "**Target:** `trip_duration` (in minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Path to dataset files: /home/codespace/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"elemento/nyc-yellow-taxi-trip-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yellow_tripdata_2015-01.csv',\n",
       " 'yellow_tripdata_2016-01.csv',\n",
       " 'yellow_tripdata_2016-02.csv',\n",
       " 'yellow_tripdata_2016-03.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"/home/codespace/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2\"\n",
    "\n",
    "sorted(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = f\"{path}/yellow_tripdata_2016-01.csv\"\n",
    "\n",
    "chunks = pd.read_csv(\n",
    "    file,\n",
    "    chunksize=500_000,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df1 = next(chunks)\n",
    "df2 = next(chunks)\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1.10</td>\n",
       "      <td>-73.990372</td>\n",
       "      <td>40.734695</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.981842</td>\n",
       "      <td>40.732407</td>\n",
       "      <td>2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>4.90</td>\n",
       "      <td>-73.980782</td>\n",
       "      <td>40.729912</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.944473</td>\n",
       "      <td>40.716679</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10.54</td>\n",
       "      <td>-73.984550</td>\n",
       "      <td>40.679565</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.950272</td>\n",
       "      <td>40.788925</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>34.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-73.993469</td>\n",
       "      <td>40.718990</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.962242</td>\n",
       "      <td>40.657333</td>\n",
       "      <td>2</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.76</td>\n",
       "      <td>-73.960625</td>\n",
       "      <td>40.781330</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.977264</td>\n",
       "      <td>40.758514</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2016-01-01 00:00:00   2016-01-01 00:00:00                2   \n",
       "1         2  2016-01-01 00:00:00   2016-01-01 00:00:00                5   \n",
       "2         2  2016-01-01 00:00:00   2016-01-01 00:00:00                1   \n",
       "3         2  2016-01-01 00:00:00   2016-01-01 00:00:00                1   \n",
       "4         2  2016-01-01 00:00:00   2016-01-01 00:00:00                3   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  RatecodeID  \\\n",
       "0           1.10        -73.990372        40.734695           1   \n",
       "1           4.90        -73.980782        40.729912           1   \n",
       "2          10.54        -73.984550        40.679565           1   \n",
       "3           4.75        -73.993469        40.718990           1   \n",
       "4           1.76        -73.960625        40.781330           1   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                  N         -73.981842         40.732407             2   \n",
       "1                  N         -73.944473         40.716679             1   \n",
       "2                  N         -73.950272         40.788925             1   \n",
       "3                  N         -73.962242         40.657333             2   \n",
       "4                  N         -73.977264         40.758514             2   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0          7.5    0.5      0.5         0.0           0.0   \n",
       "1         18.0    0.5      0.5         0.0           0.0   \n",
       "2         33.0    0.5      0.5         0.0           0.0   \n",
       "3         16.5    0.0      0.5         0.0           0.0   \n",
       "4          8.0    0.0      0.5         0.0           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  \n",
       "0                    0.3           8.8  \n",
       "1                    0.3          19.3  \n",
       "2                    0.3          34.3  \n",
       "3                    0.3          17.3  \n",
       "4                    0.3           8.8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.521659</td>\n",
       "      <td>1.738462</td>\n",
       "      <td>3.240361</td>\n",
       "      <td>-72.765110</td>\n",
       "      <td>40.085274</td>\n",
       "      <td>1.048842</td>\n",
       "      <td>-72.818357</td>\n",
       "      <td>40.115370</td>\n",
       "      <td>1.440028</td>\n",
       "      <td>12.785879</td>\n",
       "      <td>0.212128</td>\n",
       "      <td>0.497038</td>\n",
       "      <td>1.531398</td>\n",
       "      <td>0.315954</td>\n",
       "      <td>0.299712</td>\n",
       "      <td>15.642105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499531</td>\n",
       "      <td>1.329592</td>\n",
       "      <td>3.989969</td>\n",
       "      <td>9.367662</td>\n",
       "      <td>5.160466</td>\n",
       "      <td>0.481473</td>\n",
       "      <td>9.164208</td>\n",
       "      <td>5.048679</td>\n",
       "      <td>0.514079</td>\n",
       "      <td>11.770428</td>\n",
       "      <td>0.264813</td>\n",
       "      <td>0.040916</td>\n",
       "      <td>2.946590</td>\n",
       "      <td>1.944062</td>\n",
       "      <td>0.012749</td>\n",
       "      <td>14.222445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-81.101891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-76.143623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-300.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-17.400000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-300.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>-73.991386</td>\n",
       "      <td>40.733456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-73.990913</td>\n",
       "      <td>40.732273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>-73.981552</td>\n",
       "      <td>40.752167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-73.979340</td>\n",
       "      <td>40.752380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>11.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>-73.964157</td>\n",
       "      <td>40.768257</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-73.959373</td>\n",
       "      <td>40.769306</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>17.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>518.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.269276</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.233334</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1463.750000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>998.140000</td>\n",
       "      <td>923.580000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1463.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             VendorID  passenger_count   trip_distance  pickup_longitude  \\\n",
       "count  1000000.000000   1000000.000000  1000000.000000    1000000.000000   \n",
       "mean         1.521659         1.738462        3.240361        -72.765110   \n",
       "std          0.499531         1.329592        3.989969          9.367662   \n",
       "min          1.000000         0.000000        0.000000        -81.101891   \n",
       "25%          1.000000         1.000000        1.070000        -73.991386   \n",
       "50%          2.000000         1.000000        1.820000        -73.981552   \n",
       "75%          2.000000         2.000000        3.530000        -73.964157   \n",
       "max          2.000000         8.000000      518.200000          0.000000   \n",
       "\n",
       "       pickup_latitude      RatecodeID  dropoff_longitude  dropoff_latitude  \\\n",
       "count   1000000.000000  1000000.000000     1000000.000000    1000000.000000   \n",
       "mean         40.085274        1.048842         -72.818357         40.115370   \n",
       "std           5.160466        0.481473           9.164208          5.048679   \n",
       "min           0.000000        1.000000         -76.143623          0.000000   \n",
       "25%          40.733456        1.000000         -73.990913         40.732273   \n",
       "50%          40.752167        1.000000         -73.979340         40.752380   \n",
       "75%          40.768257        1.000000         -73.959373         40.769306   \n",
       "max          57.269276       99.000000           0.000000         48.233334   \n",
       "\n",
       "         payment_type     fare_amount           extra         mta_tax  \\\n",
       "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
       "mean         1.440028       12.785879        0.212128        0.497038   \n",
       "std          0.514079       11.770428        0.264813        0.040916   \n",
       "min          1.000000     -300.000000       -0.500000       -0.500000   \n",
       "25%          1.000000        6.500000        0.000000        0.500000   \n",
       "50%          1.000000        9.000000        0.000000        0.500000   \n",
       "75%          2.000000       14.500000        0.500000        0.500000   \n",
       "max          4.000000     1463.750000        7.000000        0.890000   \n",
       "\n",
       "           tip_amount    tolls_amount  improvement_surcharge    total_amount  \n",
       "count  1000000.000000  1000000.000000         1000000.000000  1000000.000000  \n",
       "mean         1.531398        0.315954               0.299712       15.642105  \n",
       "std          2.946590        1.944062               0.012749       14.222445  \n",
       "min         -3.000000      -17.400000              -0.300000     -300.800000  \n",
       "25%          0.000000        0.000000               0.300000        8.000000  \n",
       "50%          1.000000        0.000000               0.300000       11.160000  \n",
       "75%          2.050000        0.000000               0.300000       17.160000  \n",
       "max        998.140000      923.580000               0.300000     1463.750000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Advanced Feature Engineering for NYC Taxi Data\n",
    "\n",
    "For NYC taxi data, we can create meaningful features based on:\n",
    "1. **Geospatial features**: Distance, direction, borough information\n",
    "2. **Temporal features**: Time of day, day type, seasonality\n",
    "3. **Interaction features**: Combining distance with time, passenger effects\n",
    "4. **Statistical features**: Speed estimates, efficiency metrics\n",
    "\n",
    "Our custom transformer will create these domain-specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# NEW: Advanced Feature Engineering Class for NYC Taxi\n",
    "class NYCFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Advanced feature engineering for NYC taxi data using domain knowledge\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting needed for this transformer\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform input data with engineered features\"\"\"\n",
    "        X_df = pd.DataFrame(X, columns=feature_names)\n",
    "        \n",
    "        # DOMAIN-DRIVEN FEATURE ENGINEERING:\n",
    "        \n",
    "        # 1. Haversine distance (more accurate than Euclidean)\n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            \"\"\"Calculate haversine distance between two points\"\"\"\n",
    "            R = 3958.8  # Earth radius in miles\n",
    "            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "            c = 2 * np.arcsin(np.sqrt(a))\n",
    "            return R * c\n",
    "        \n",
    "        haversine_dist = haversine_distance(\n",
    "            X_df['pickup_latitude'], X_df['pickup_longitude'],\n",
    "            X_df['dropoff_latitude'], X_df['dropoff_longitude']\n",
    "        )\n",
    "        \n",
    "        # 2. Direction features (angle)\n",
    "        delta_lat = X_df['dropoff_latitude'] - X_df['pickup_latitude']\n",
    "        delta_lon = X_df['dropoff_longitude'] - X_df['pickup_longitude']\n",
    "        direction_angle = np.arctan2(delta_lat, delta_lon)\n",
    "        \n",
    "        # 3. Manhattan distance approximation (NYC streets are grid-like)\n",
    "        manhattan_dist = (np.abs(delta_lat) * 69 + np.abs(delta_lon) * 53)  # Approx conversion to miles\n",
    "        \n",
    "        # 4. Temporal features\n",
    "        is_rush_hour = ((X_df['pickup_hour'] >= 7) & (X_df['pickup_hour'] <= 9)) | \\\n",
    "                       ((X_df['pickup_hour'] >= 16) & (X_df['pickup_hour'] <= 18))\n",
    "        is_night = (X_df['pickup_hour'] >= 22) | (X_df['pickup_hour'] <= 5)\n",
    "        is_weekend = X_df['pickup_dayofweek'].isin([5, 6])\n",
    "        \n",
    "        # 5. Centrality features (distance from NYC center - Times Square)\n",
    "        times_square_lat, times_square_lon = 40.7580, -73.9855\n",
    "        pickup_from_center = haversine_distance(\n",
    "            X_df['pickup_latitude'], X_df['pickup_longitude'],\n",
    "            times_square_lat, times_square_lon\n",
    "        )\n",
    "        dropoff_from_center = haversine_distance(\n",
    "            X_df['dropoff_latitude'], X_df['dropoff_longitude'],\n",
    "            times_square_lat, times_square_lon\n",
    "        )\n",
    "        \n",
    "        # 6. Efficiency ratio (straight line vs actual)\n",
    "        efficiency_ratio = haversine_dist / (X_df['distance'] + 1e-8)\n",
    "        \n",
    "        # 7. Passenger efficiency (distance per passenger)\n",
    "        distance_per_passenger = X_df['distance'] / (X_df['passenger_count'] + 1e-8)\n",
    "        \n",
    "        # 8. Hour as cyclical feature\n",
    "        hour_sin = np.sin(2 * np.pi * X_df['pickup_hour'] / 24)\n",
    "        hour_cos = np.cos(2 * np.pi * X_df['pickup_hour'] / 24)\n",
    "        \n",
    "        # Combine all features\n",
    "        X_eng = np.column_stack([\n",
    "            X,  # Original features\n",
    "            haversine_dist.values.reshape(-1, 1),\n",
    "            direction_angle.values.reshape(-1, 1),\n",
    "            manhattan_dist.values.reshape(-1, 1),\n",
    "            is_rush_hour.values.reshape(-1, 1).astype(float),\n",
    "            is_night.values.reshape(-1, 1).astype(float),\n",
    "            is_weekend.values.reshape(-1, 1).astype(float),\n",
    "            pickup_from_center.values.reshape(-1, 1),\n",
    "            dropoff_from_center.values.reshape(-1, 1),\n",
    "            efficiency_ratio.values.reshape(-1, 1),\n",
    "            distance_per_passenger.values.reshape(-1, 1),\n",
    "            hour_sin.values.reshape(-1, 1),\n",
    "            hour_cos.values.reshape(-1, 1)\n",
    "        ])\n",
    "        \n",
    "        # Update feature names for interpretability\n",
    "        self.feature_names = feature_names + [\n",
    "            'haversine_distance', 'direction_angle', 'manhattan_distance',\n",
    "            'is_rush_hour', 'is_night', 'is_weekend',\n",
    "            'pickup_from_center', 'dropoff_from_center',\n",
    "            'efficiency_ratio', 'distance_per_passenger',\n",
    "            'hour_sin', 'hour_cos'\n",
    "        ]\n",
    "        \n",
    "        return X_eng\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Mimics sklearn's fit_transform\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "        \n",
    "# Test our feature engineering\n",
    "engineer = NYCFeatureEngineer()\n",
    "X_engineered = engineer.fit_transform(X)\n",
    "\n",
    "print(f\"\\nüéØ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(f\"‚Ä¢ Original features: {X.shape[1]}\")\n",
    "print(f\"‚Ä¢ Engineered features: {X_engineered.shape[1]} (NEW!)\")\n",
    "print(f\"‚Ä¢ New features created: {engineer.feature_names[-12:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß≠ Outlier Handling with IQR ‚Äî Making the Data More Robust\n",
    "\n",
    "Taxi data often contains outliers due to GPS errors, incorrect entries, or rare but valid long trips. We'll use IQR-based outlier handling to make our models more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Outlier Handler for Robust Models\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handle outliers using IQR method - More robust than simple scaling\"\"\"\n",
    "    \n",
    "    def __init__(self, factor=1.5):\n",
    "        self.factor = factor\n",
    "        self.lower_bounds_ = None\n",
    "        self.upper_bounds_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bounds_ = []\n",
    "        self.upper_bounds_ = []\n",
    "        \n",
    "        # Calculate IQR bounds for each feature\n",
    "        for i in range(X.shape[1]):\n",
    "            Q1 = np.percentile(X[:, i], 25)  # 25th percentile\n",
    "            Q3 = np.percentile(X[:, i], 75)  # 75th percentile  \n",
    "            IQR = Q3 - Q1  # Interquartile Range\n",
    "            self.lower_bounds_.append(Q1 - self.factor * IQR)\n",
    "            self.upper_bounds_.append(Q3 + self.factor * IQR)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        # Clip values to IQR bounds\n",
    "        for i in range(X.shape[1]):\n",
    "            lower = self.lower_bounds_[i]\n",
    "            upper = self.upper_bounds_[i]\n",
    "            X_transformed[:, i] = np.clip(X_transformed[:, i], lower, upper)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Building the Final Preprocessing Pipeline\n",
    "\n",
    "We'll combine all steps into a single `Pipeline` object for consistency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive preprocessing pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('feature_engineer', NYCFeatureEngineer()),  # Our new taxi-specific features\n",
    "    ('outlier_handler', OutlierHandler(factor=1.5)),  # Handle outliers\n",
    "    ('scaler', RobustScaler())  # Robust to outliers (better than StandardScaler)\n",
    "])\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "print(\"üîÑ Applying preprocessing pipeline...\")\n",
    "X_processed = preprocessor.fit_transform(X, y)\n",
    "\n",
    "print(\"‚úÖ ADVANCED PREPROCESSING PIPELINE BUILT!\")\n",
    "print(f\"üìä Processed data shape: {X_processed.shape}\")\n",
    "print(f\"üéØ Number of features: {len(preprocessor.named_steps['feature_engineer'].get_feature_names())}\")\n",
    "print(\"\\nüìã All feature names:\")\n",
    "for i, feat in enumerate(preprocessor.named_steps['feature_engineer'].get_feature_names()):\n",
    "    print(f\"  {i+1:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Smarter Data Splitting: Adding a Validation Set\n",
    "\n",
    "We'll use train/validation/test splits for better model evaluation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved data splitting with validation set\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_processed, y, test_size=config.TEST_SIZE, random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=config.VAL_SIZE, random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"üìä DATA SPLITS (IMPROVED from previous notebook):\")\n",
    "print(f\"‚Ä¢ Training: {X_train.shape[0]:,} samples (model learning)\")\n",
    "print(f\"‚Ä¢ Validation: {X_val.shape[0]:,} samples (hyperparameter tuning)\") \n",
    "print(f\"‚Ä¢ Test: {X_test.shape[0]:,} samples (final evaluation - NEVER TOUCHED until end)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Expanding Our Model Arsenal: From Basics to Advanced\n",
    "\n",
    "We'll use a diverse set of models to find the best performer for taxi duration prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define advanced models - Expanded from previous work\n",
    "advanced_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(random_state=config.RANDOM_STATE, alpha=1.0),\n",
    "    'Lasso Regression': Lasso(random_state=config.RANDOM_STATE, alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(random_state=config.RANDOM_STATE, alpha=0.1, l1_ratio=0.5),  # NEW: Combines L1 + L2\n",
    "    'Random Forest': RandomForestRegressor(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=config.RANDOM_STATE, n_estimators=100),  # NEW: Sequential learning\n",
    "    'Support Vector Regression': SVR(kernel='rbf', C=1.0, epsilon=0.1),  # NEW: Different approach\n",
    "}\n",
    "\n",
    "# NEW: Voting Ensemble - Combines multiple models\n",
    "voting_ensemble = VotingRegressor([\n",
    "    ('ridge', Ridge(random_state=config.RANDOM_STATE, alpha=1.0)),\n",
    "    ('rf', RandomForestRegressor(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS, n_estimators=100)),\n",
    "    ('gb', GradientBoostingRegressor(random_state=config.RANDOM_STATE, n_estimators=100))\n",
    "])\n",
    "\n",
    "advanced_models['Voting Ensemble'] = voting_ensemble\n",
    "\n",
    "print(f\"\\nüéØ MODEL PORTFOLIO ({len(advanced_models)} models):\")\n",
    "for i, (name, model) in enumerate(advanced_models.items(), 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Step-by-Step Model Training and Logging with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# üì¶ Helper 1 ‚Äî Basic training and evaluation\n",
    "# ===========================\n",
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train model and compute basic metrics on train and validation sets.\"\"\"\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        \"val_rmse\": np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        \"train_r2\": r2_score(y_train, y_train_pred),\n",
    "        \"val_r2\": r2_score(y_val, y_val_pred),\n",
    "        \"train_mae\": mean_absolute_error(y_train, y_train_pred),\n",
    "        \"val_mae\": mean_absolute_error(y_val, y_val_pred),\n",
    "        \"training_time\": training_time,\n",
    "        \"overfitting_gap\": r2_score(y_train, y_train_pred) - r2_score(y_val, y_val_pred) # EARLY DETECTION FOR OVERFITTING\n",
    "    }\n",
    "\n",
    "    return metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# üì¶ Helper 2 ‚Äî Cross-validation\n",
    "# ===========================\n",
    "def compute_cross_validation(model, X_train, y_train, cv_folds=config.CV_FOLDS):\n",
    "    \"\"\"Run cross-validation and return mean and std of R¬≤ scores.\"\"\"\n",
    "    cv_scores = cross_val_score(model, X_train, y_train,\n",
    "                                cv=cv_folds, scoring='r2', n_jobs=config.N_JOBS)\n",
    "    return cv_scores.mean(), cv_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# üì¶ Helper 3 ‚Äî MLflow Logging\n",
    "# ===========================\n",
    "def log_to_mlflow(model, metrics, cv_mean, cv_std, run_name):\n",
    "    \"\"\"Log params, metrics, and model to MLflow in a clean, minimal way.\"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # 1. Log hyperparameters\n",
    "        mlflow.log_params(model.get_params())\n",
    "        \n",
    "        # 2. Log main metrics\n",
    "        for k, v in metrics.items():\n",
    "            if k != 'training_time':  # avoid logging long times directly\n",
    "                mlflow.log_metric(k, float(v))\n",
    "        mlflow.log_metric(\"cv_r2_mean\", float(cv_mean))\n",
    "        mlflow.log_metric(\"cv_r2_std\", float(cv_std))\n",
    "        \n",
    "        # 3. Save model artifact\n",
    "        mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# üì¶ Helper 4 ‚Äî Advanced Model Evaluation\n",
    "# ===========================\n",
    "def evaluate_model_advanced(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"Train, evaluate, cross-validate, and log model in a clean step-by-step way.\"\"\"\n",
    "    # 1. Train and evaluate\n",
    "    metrics, trained_model = train_and_evaluate(model, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # 2. Cross-validation\n",
    "    cv_mean, cv_std = compute_cross_validation(model, X_train, y_train)\n",
    "    metrics[\"cv_r2_mean\"] = cv_mean\n",
    "    metrics[\"cv_r2_std\"] = cv_std\n",
    "    \n",
    "    # 3. Log everything to MLflow\n",
    "    log_to_mlflow(model, metrics, cv_mean, cv_std, model_name)\n",
    "    \n",
    "    return metrics, trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Run & Track All Advanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ STARTING ADVANCED MODEL EVALUATION...\")\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in advanced_models.items():\n",
    "    print(f\"\\nüîß Training {name}...\")\n",
    "    metrics, trained_model = evaluate_model_advanced(model, X_train, X_val, y_train, y_val, name)\n",
    "    \n",
    "    results[name] = metrics\n",
    "    trained_models[name] = trained_model\n",
    "\n",
    "    overfit_flag = \"‚ö†Ô∏è\" if metrics['overfitting_gap'] > 0.1 else \"‚úÖ\"\n",
    "    print(f\"‚úÖ {name:20} | Val R¬≤: {metrics['val_r2']:.4f} | \"\n",
    "          f\"CV R¬≤: {metrics['cv_r2_mean']:.4f} ¬± {metrics['cv_r2_std']:.4f} {overfit_flag}\")\n",
    "\n",
    "print(\"\\nüìà All models trained and logged to MLflow!\")\n",
    "print(f\"üí° Launch MLflow UI with: mlflow ui --backend-store-uri {config.EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Model Comparison & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 1Ô∏è‚É£ Convert results dict to DataFrame\n",
    "# ===========================\n",
    "metrics_df = pd.DataFrame(results).T  # transpose so models are rows\n",
    "metrics_df = metrics_df[['val_r2', 'val_rmse', 'val_mae', 'overfitting_gap', 'cv_r2_mean']]\n",
    "metrics_df = metrics_df.sort_values('val_r2', ascending=False)\n",
    "metrics_df = metrics_df.reset_index().rename(columns={'index': 'Model'})\n",
    "\n",
    "print(\"üìä Model comparison table (sorted by Validation R¬≤):\")\n",
    "display(metrics_df)\n",
    "\n",
    "# ===========================\n",
    "# 2Ô∏è‚É£ Plot Validation R¬≤ (Higher is better)\n",
    "# ===========================\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=metrics_df, x='Model', y='val_r2', palette='viridis')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Validation R¬≤ Comparison - NYC Taxi Duration Prediction (Higher is Better ‚úÖ)')\n",
    "plt.ylabel('Validation R¬≤')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "# ===========================\n",
    "# 3Ô∏è‚É£ Plot Validation RMSE (Lower is better)\n",
    "# ===========================\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=metrics_df, x='Model', y='val_rmse', palette='magma')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Validation RMSE Comparison - NYC Taxi Duration (Lower is Better ‚úÖ)')\n",
    "plt.ylabel('Validation RMSE (minutes)')\n",
    "plt.xlabel('')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "# ===========================\n",
    "# 4Ô∏è‚É£ Plot Overfitting Gap (Closer to 0 is better)\n",
    "# ===========================\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if gap > 0.1 else 'green' for gap in metrics_df['overfitting_gap']]\n",
    "bars = plt.bar(metrics_df['Model'], metrics_df['overfitting_gap'], color=colors, alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Overfitting Gap (train R¬≤ - val R¬≤) - NYC Taxi Duration (Closer to 0 is Better ‚öñÔ∏è)')\n",
    "plt.ylabel('Overfitting Gap')\n",
    "plt.xlabel('')\n",
    "plt.axhline(y=0.1, color='red', linestyle='--', alpha=0.8, label='Overfitting Threshold')\n",
    "plt.axhline(y=-0.1, color='blue', linestyle='--', alpha=0.8, label='Underfitting Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Advanced Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive hyperparameter grids for NYC Taxi data\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200, 300],  # Number of trees\n",
    "        'max_depth': [None, 10, 20, 30, 40],       # Tree depth\n",
    "        'min_samples_split': [2, 5, 10, 20],       # Minimum samples to split\n",
    "        'min_samples_leaf': [1, 2, 4, 8],         # Minimum samples per leaf\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Features to consider for splits\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200, 300],        # Number of boosting stages\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],  # Step size shrinkage\n",
    "        'max_depth': [3, 4, 5, 6, 7],             # Maximum depth per tree\n",
    "        'min_samples_split': [2, 5, 10, 20],       # Minimum samples to split\n",
    "        'subsample': [0.8, 0.9, 1.0]           # Fraction of samples for fitting\n",
    "    },\n",
    "    \n",
    "    'Ridge Regression': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],  # Regularization strength\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']  # Algorithm\n",
    "    },\n",
    "    \n",
    "    'Voting Ensemble': {\n",
    "        'ridge__alpha': [0.1, 1.0, 10.0, 100.0],\n",
    "        'rf__n_estimators': [50, 100, 200],\n",
    "        'rf__max_depth': [10, 20, 30],\n",
    "        'gb__n_estimators': [50, 100, 200],\n",
    "        'gb__learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
    "    }\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter optimization\n",
    "print(\"üéØ STARTING HYPERPARAMETER OPTIMIZATION...\")\n",
    "tuned_models = {}\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name in ['Random Forest', 'Gradient Boosting', 'Ridge Regression', 'Voting Ensemble']:\n",
    "    print(f\"\\nüîß Tuning {model_name}...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{model_name}_tuned_nyc\"):\n",
    "        # Use RandomizedSearchCV for efficient optimization\n",
    "        search = RandomizedSearchCV(\n",
    "            advanced_models[model_name],\n",
    "            param_grids[model_name],\n",
    "            n_iter=30,  # Try 30 random combinations (efficient!)\n",
    "            cv=config.CV_FOLDS,\n",
    "            scoring='r2',\n",
    "            n_jobs=config.N_JOBS,\n",
    "            random_state=config.RANDOM_STATE,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Perform the search\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        # Store results\n",
    "        tuned_models[model_name] = search.best_estimator_\n",
    "        optimization_results[model_name] = {\n",
    "            'best_score': search.best_score_,\n",
    "            'best_params': search.best_params_,\n",
    "            'best_estimator': search.best_estimator_\n",
    "        }\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(search.best_params_)\n",
    "        mlflow.log_metric('best_cv_score', search.best_score_)\n",
    "        mlflow.sklearn.log_model(search.best_estimator_, \"tuned_model\")\n",
    "        \n",
    "        print(f\"‚úÖ {model_name:20} | Best CV R¬≤: {search.best_score_:.4f}\")\n",
    "        print(f\"   Best parameters: {search.best_params_}\")\n",
    "\n",
    "print(f\"\\nüéâ HYPERPARAMETER OPTIMIZATION COMPLETE!\")\n",
    "print(f\"üí° All tuned models saved in MLflow for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Tuned vs Untuned Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# ===========================\n",
    "# 1Ô∏è‚É£ Evaluate tuned models on validation set\n",
    "# ===========================\n",
    "tuned_results = {}\n",
    "for model_name, tuned_model in tuned_models.items():\n",
    "    y_val_pred = tuned_model.predict(X_val)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    improvement = val_r2 - results[model_name]['val_r2']  # vs untuned\n",
    "    \n",
    "    tuned_results[model_name] = {\n",
    "        'val_r2': val_r2,\n",
    "        'val_rmse': val_rmse,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "# ===========================\n",
    "# 2Ô∏è‚É£ Print simple comparison table\n",
    "# ===========================\n",
    "print(f\"{'Model':<20} {'Untuned R¬≤':<12} {'Tuned R¬≤':<12} {'Improvement':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for model_name, res in tuned_results.items():\n",
    "    untuned_r2 = results[model_name]['val_r2']\n",
    "    tuned_r2 = res['val_r2']\n",
    "    improvement = res['improvement']\n",
    "    icon = \"üìà\" if improvement > 0.001 else \"üìâ\" if improvement < -0.001 else \"‚û°Ô∏è\"\n",
    "    print(f\"{model_name:<20} {untuned_r2:>10.4f} {tuned_r2:>10.4f} {icon} {improvement:>8.4f}\")\n",
    "\n",
    "# ===========================\n",
    "# 3Ô∏è‚É£ Plot Tuned vs Untuned R¬≤\n",
    "# ===========================\n",
    "models = list(tuned_results.keys())\n",
    "untuned_r2 = [results[m]['val_r2'] for m in models]\n",
    "tuned_r2 = [tuned_results[m]['val_r2'] for m in models]\n",
    "improvement = [tuned_results[m]['improvement'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "\n",
    "# R¬≤ comparison\n",
    "axes[0].bar(x - width/2, untuned_r2, width, label='Untuned', alpha=0.7, color='lightblue')\n",
    "axes[0].bar(x + width/2, tuned_r2, width, label='Tuned', alpha=0.7, color='lightgreen')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Validation R¬≤')\n",
    "axes[0].set_title('Untuned vs Tuned R¬≤ - NYC Taxi Duration (Higher is Better)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Improvement plot\n",
    "colors = ['green' if i>0.001 else 'red' if i<-0.001 else 'gray' for i in improvement]\n",
    "axes[1].bar(models, improvement, color=colors, alpha=0.7)\n",
    "axes[1].axhline(0, color='black', linestyle='--', alpha=0.8)\n",
    "axes[1].axhline(0.01, color='green', linestyle=':', alpha=0.5, label='Significant Improvement')\n",
    "axes[1].axhline(-0.01, color='red', linestyle=':', alpha=0.5, label='Significant Degradation')\n",
    "axes[1].set_ylabel('R¬≤ Improvement')\n",
    "axes[1].set_title('Tuning Impact - NYC Taxi Duration (Green=Better, Red=Worse)')\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===========================\n",
    "# 4Ô∏è‚É£ Best tuned model\n",
    "# ===========================\n",
    "best_model_name = max(tuned_results, key=lambda m: tuned_results[m]['val_r2'])\n",
    "best_tuned_model = tuned_models[best_model_name]\n",
    "print(f\"\\nüèÜ BEST TUNED MODEL: {best_model_name}\")\n",
    "print(f\"üìä Validation R¬≤: {tuned_results[best_model_name]['val_r2']:.4f}\")\n",
    "print(f\"üìà Improvement over untuned: +{tuned_results[best_model_name]['improvement']:.4f}\")\n",
    "print(f\"üîß Best parameters: {optimization_results[best_model_name]['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = best_tuned_model.predict(X_test)\n",
    "\n",
    "# Compute performance metrics\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"üî¨ FINAL TEST SET EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Test R¬≤: {test_r2:.4f}\")\n",
    "print(f\"üìä Test RMSE: {test_rmse:.4f} minutes\")\n",
    "print(f\"üìä Test MAE: {test_mae:.4f} minutes\")\n",
    "print(f\"\\nüìà For a typical 15-minute taxi ride:\")\n",
    "print(f\"   ‚Ä¢ Average error: ¬±{test_mae:.1f} minutes ({test_mae/15*100:.1f}% of trip duration)\")\n",
    "print(f\"   ‚Ä¢ RMSE error: ¬±{test_rmse:.1f} minutes ({test_rmse/15*100:.1f}% of trip duration)\")\n",
    "print(f\"\\n‚úÖ Model explains {test_r2*100:.1f}% of variance in taxi trip durations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# ===========================\n",
    "# 0Ô∏è‚É£ Prepare feature names\n",
    "# ===========================\n",
    "feature_names_all = preprocessor.named_steps['feature_engineer'].get_feature_names()\n",
    "\n",
    "# ===========================\n",
    "# 1Ô∏è‚É£ Identify best tuned model\n",
    "# ===========================\n",
    "print(f\"üèÜ BEST TUNED MODEL: {best_model_name}\")\n",
    "print(f\"üìä Validation R¬≤: {tuned_results[best_model_name]['val_r2']:.4f}\")\n",
    "print(f\"üìä Test R¬≤: {test_r2:.4f}\")\n",
    "\n",
    "# ===========================\n",
    "# 2Ô∏è‚É£ Versioning and saving\n",
    "# ===========================\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_version = f\"nyc_taxi_v1_{timestamp}\"\n",
    "model_save_dir = os.path.join(config.MODEL_DIR, model_version)\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(model_save_dir, 'best_model.pkl')\n",
    "joblib.dump(best_tuned_model, model_path)\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "# Save preprocessing pipeline\n",
    "preprocessor_path = os.path.join(model_save_dir, 'preprocessor.pkl')\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"‚úÖ Preprocessor saved: {preprocessor_path}\")\n",
    "\n",
    "# ===========================\n",
    "# 3Ô∏è‚É£ Create model card\n",
    "# ===========================\n",
    "model_card = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_version': model_version,\n",
    "    'timestamp': timestamp,\n",
    "    'dataset': 'NYC Taxi Trip Duration',\n",
    "    'target': 'trip_duration (minutes)',\n",
    "    \n",
    "    'performance': {\n",
    "        'test_r2': float(test_r2),\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'test_mae': float(test_mae),\n",
    "        'val_r2': float(tuned_results[best_model_name]['val_r2']),\n",
    "        'best_cv_score': float(optimization_results[best_model_name]['best_score']),\n",
    "    },\n",
    "    \n",
    "    'data_info': {\n",
    "        'total_samples': int(len(X)),\n",
    "        'train_samples': int(len(X_train)),\n",
    "        'val_samples': int(len(X_val)),\n",
    "        'test_samples': int(len(X_test)),\n",
    "        'n_features': X_test.shape[1],\n",
    "        'original_features': feature_names,\n",
    "        'engineered_features': feature_names_all[len(feature_names):],\n",
    "    },\n",
    "    \n",
    "    'model_config': {\n",
    "        'model_class': best_model_name,\n",
    "        'hyperparameters': dict(best_tuned_model.get_params()) \n",
    "            if hasattr(best_tuned_model, 'get_params') else {},\n",
    "    },\n",
    "    \n",
    "    'preprocessing': {\n",
    "        'steps': [\n",
    "            'NYCFeatureEngineering (12 new taxi-specific features)',\n",
    "            'OutlierHandling (IQR method with factor=1.5)',\n",
    "            'RobustScaler (outlier-resistant scaling)',\n",
    "        ],\n",
    "        'engineered_features': [\n",
    "            'haversine_distance', 'direction_angle', 'manhattan_distance',\n",
    "            'is_rush_hour', 'is_night', 'is_weekend',\n",
    "            'pickup_from_center', 'dropoff_from_center',\n",
    "            'efficiency_ratio', 'distance_per_passenger',\n",
    "            'hour_sin', 'hour_cos'\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    'deployment': {\n",
    "        'status': 'Ready for Production',\n",
    "        'recommendations': [\n",
    "            'Monitor prediction errors in production',\n",
    "            'Retrain monthly with new taxi data',\n",
    "            'Alert if RMSE exceeds 8 minutes',\n",
    "            'Consider time-of-day and traffic patterns for better accuracy'\n",
    "        ],\n",
    "        'expected_performance': {\n",
    "            'r2_range': f'{test_r2*100:.1f}% variance explained',\n",
    "            'mae_range': f'¬±{test_mae:.1f} minutes',\n",
    "            'rmse_range': f'¬±{test_rmse:.1f} minutes'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "card_path = os.path.join(model_save_dir, 'model_card.json')\n",
    "with open(card_path, 'w') as f:\n",
    "    json.dump(model_card, f, indent=2)\n",
    "print(f\"‚úÖ Model card saved: {card_path}\")\n",
    "\n",
    "# ===========================\n",
    "# 4Ô∏è‚É£ Deployment requirements\n",
    "# ===========================\n",
    "requirements = {\n",
    "    'python': '3.8+',\n",
    "    'packages': {\n",
    "        'scikit-learn': '1.0+',\n",
    "        'numpy': '1.20+',\n",
    "        'pandas': '1.3+',\n",
    "        'joblib': '1.0+',\n",
    "        'mlflow': '1.0+',\n",
    "    }\n",
    "}\n",
    "\n",
    "req_path = os.path.join(model_save_dir, 'requirements.json')\n",
    "with open(req_path, 'w') as f:\n",
    "    json.dump(requirements, f, indent=2)\n",
    "print(f\"‚úÖ Deployment requirements saved: {req_path}\")\n",
    "\n",
    "# ===========================\n",
    "# 5Ô∏è‚É£ Create example prediction script\n",
    "# ===========================\n",
    "example_script = '''# Example: Making predictions with the NYC Taxi Duration model\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and preprocessor\n",
    "model = joblib.load('best_model.pkl')\n",
    "preprocessor = joblib.load('preprocessor.pkl')\n",
    "\n",
    "# Example input (single trip)\n",
    "example_trip = pd.DataFrame({\n",
    "    'pickup_longitude': [-73.9855],\n",
    "    'pickup_latitude': [40.7580],\n",
    "    'dropoff_longitude': [-73.9772],\n",
    "    'dropoff_latitude': [40.7527],\n",
    "    'passenger_count': [2],\n",
    "    'vendor_id': [1],\n",
    "    'pickup_hour': [17],  # 5 PM - rush hour\n",
    "    'pickup_dayofweek': [2],  # Tuesday\n",
    "    'distance': [2.5]  # miles\n",
    "})\n",
    "\n",
    "# Preprocess and predict\n",
    "X_processed = preprocessor.transform(example_trip.values)\n",
    "predicted_duration = model.predict(X_processed)\n",
    "\n",
    "print(f\"Predicted trip duration: {predicted_duration[0]:.1f} minutes\")\n",
    "'''\n",
    "\n",
    "script_path = os.path.join(model_save_dir, 'example_prediction.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(example_script)\n",
    "print(f\"‚úÖ Example prediction script saved: {script_path}\")\n",
    "\n",
    "# ===========================\n",
    "# 6Ô∏è‚É£ Summary\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üíæ DEPLOYMENT PACKAGE READY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Location: {model_save_dir}\")\n",
    "print(f\"\\nüì¶ Contents:\")\n",
    "print(f\"  1. best_model.pkl - Trained {best_model_name}\")\n",
    "print(f\"  2. preprocessor.pkl - Complete preprocessing pipeline\")\n",
    "print(f\"  3. model_card.json - Model metadata and performance\")\n",
    "print(f\"  4. requirements.json - Package dependencies\")\n",
    "print(f\"  5. example_prediction.py - Example usage script\")\n",
    "print(f\"\\nüöÄ Ready for deployment in production or Streamlit app!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Key Learnings Recap\n",
    "\n",
    "This notebook represents a complete production-ready workflow for NYC Taxi Duration prediction. Here's what we accomplished:\n",
    "\n",
    "1. **‚úÖ Domain-Specific Feature Engineering**  \n",
    "   - Created 12 taxi-specific features (Haversine distance, rush hour flags, cyclical time features, etc.)\n",
    "   - Implemented custom transformer for reproducibility\n",
    "\n",
    "2. **‚úÖ Robust Data Pipeline**  \n",
    "   - IQR-based outlier handling for taxi data anomalies\n",
    "   - Robust scaling for better model stability\n",
    "   - Complete preprocessing pipeline\n",
    "\n",
    "3. **‚úÖ Comprehensive Model Evaluation**  \n",
    "   - Compared 8 different regression models\n",
    "   - Used cross-validation for reliable performance estimates\n",
    "   - Tracked overfitting with train-val gap analysis\n",
    "\n",
    "4. **‚úÖ MLflow Experiment Tracking**  \n",
    "   - Logged all experiments, parameters, and metrics\n",
    "   - Saved model artifacts for reproducibility\n",
    "   - Enabled easy comparison of different approaches\n",
    "\n",
    "5. **‚úÖ Hyperparameter Optimization**  \n",
    "   - Tuned 4 key models using RandomizedSearchCV\n",
    "   - Achieved performance improvements through systematic search\n",
    "   - Identified best model configuration\n",
    "\n",
    "6. **‚úÖ Production Deployment Preparation**  \n",
    "   - Model versioning with timestamp\n",
    "   - Complete model card with metadata\n",
    "   - Saved pipeline and dependencies\n",
    "   - Example prediction script\n",
    "\n",
    "**üí° Summary:**  \n",
    "This workflow demonstrates how to take real-world taxi data from raw features to a **production-ready prediction system**. The model can now be integrated into applications, dashboards, or APIs for real-time taxi duration predictions.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Create a Streamlit app for interactive predictions\n",
    "2. Deploy as a REST API using FastAPI or Flask\n",
    "3. Set up monitoring for model performance drift\n",
    "4. Implement A/B testing for model updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è Exercise: Extend the Workflow\n",
    "\n",
    "### üéØ Objective\n",
    "Extend this workflow with the following enhancements:\n",
    "\n",
    "1. **Feature Importance Analysis**\n",
    "   - Use permutation importance or SHAP values\n",
    "   - Identify which features most influence trip duration\n",
    "\n",
    "2. **Error Analysis**\n",
    "   - Analyze where the model makes largest errors\n",
    "   - Investigate patterns in under/over predictions\n",
    "\n",
    "3. **Business Metrics**\n",
    "   - Convert RMSE to business impact (e.g., cost implications)\n",
    "   - Calculate confidence intervals for predictions\n",
    "\n",
    "4. **Deployment Script**\n",
    "   - Create a complete Streamlit app for taxi duration prediction\n",
    "   - Add visualizations and explanations\n",
    "\n",
    "### üìù Instructions\n",
    "Choose one or more enhancements and implement them in separate notebooks or scripts.\n",
    "\n",
    "### üìÇ Deliverables\n",
    "1. Extended analysis notebook\n",
    "2. Streamlit app code\n",
    "3. Short video demo of the app\n",
    "\n",
    "> **Goal:** Demonstrate how to take a production ML model and add real business value through analysis, interpretation, and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéâ CONGRATULATIONS! You've completed the NYC Taxi Duration Production ML Project!\n",
    "# Your model is now ready for deployment and real-world use.\n",
    "\n",
    "print(\"\\n\" + \"üéâ\" * 30)\n",
    "print(\"  NYC TAXI DURATION PREDICTION PROJECT COMPLETE!\")\n",
    "print(\"üéâ\" * 30)\n",
    "\n",
    "print(\"\\nüìä FINAL RESULTS SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Best Model: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Test R¬≤: {test_r2:.4f} ({test_r2*100:.1f}% variance explained)\")\n",
    "print(f\"   ‚Ä¢ Test RMSE: {test_rmse:.2f} minutes\")\n",
    "print(f\"   ‚Ä¢ Test MAE: {test_mae:.2f} minutes\")\n",
    "print(f\"\\nüíæ Model saved to: {model_save_dir}\")\n",
    "print(f\"üìà MLflow experiments: {config.EXPERIMENT_DIR}\")\n",
    "print(f\"\\nüöÄ Ready for production deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
