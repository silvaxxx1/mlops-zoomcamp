{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöñ NYC Yellow Taxi Trip Duration Prediction: PRODUCTION-READY (No Data Leakage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Production Environment Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è Advanced imports for production ML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üîß Core Python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# üß∞ Sklearn libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# üß™ MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Configuration & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/03 16:59:20 INFO mlflow.tracking.fluent: Experiment with name 'nyc_taxi_no_leakage_production' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration initialized!\n",
      "üìÅ Model directory: models_nyc_taxi_no_leakage\n",
      "üìÅ Experiment directory: experiments_nyc_taxi_no_leakage\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Reproducibility\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.2\n",
    "    CV_FOLDS = 5\n",
    "    N_JOBS = -1\n",
    "    \n",
    "    # Model directories\n",
    "    MODEL_DIR = \"models_nyc_taxi_no_leakage\"\n",
    "    EXPERIMENT_DIR = \"experiments_nyc_taxi_no_leakage\"\n",
    "    DATA_DIR = \"data\"\n",
    "    \n",
    "    # Data processing - MORE CONSERVATIVE for real predictions\n",
    "    SAMPLE_SIZE = 200000\n",
    "    MIN_TRIP_DURATION = 60  # 1 minute in seconds\n",
    "    MAX_TRIP_DURATION = 7200  # 2 hours in seconds\n",
    "    MIN_TRIP_DISTANCE = 0.1  # miles\n",
    "    MAX_TRIP_DISTANCE = 50  # miles\n",
    "    \n",
    "    # NYC coordinates bounds\n",
    "    NYC_LAT_RANGE = (40.5, 40.9)\n",
    "    NYC_LON_RANGE = (-74.3, -73.7)\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_path in [MODEL_DIR, EXPERIMENT_DIR, DATA_DIR]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Initialize MLflow\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(config.EXPERIMENT_DIR)}\")\n",
    "experiment_name = \"nyc_taxi_no_leakage_production\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"‚úÖ Configuration initialized!\")\n",
    "print(f\"üìÅ Model directory: {config.MODEL_DIR}\")\n",
    "print(f\"üìÅ Experiment directory: {config.EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to /home/silva/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/2.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%| | 50.0M/1.78G [01:09<41:15, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m path = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43melemento/nyc-yellow-taxi-trip-data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPath to dataset files:\u001b[39m\u001b[33m\"\u001b[39m, path)\n\u001b[32m      9\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/home/codespace/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/kagglehub/datasets.py:45\u001b[39m, in \u001b[36mdataset_download\u001b[39m\u001b[34m(handle, path, force_download)\u001b[39m\n\u001b[32m     43\u001b[39m h = parse_dataset_handle(handle)\n\u001b[32m     44\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh.to_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, extra={**EXTRA_CONSOLE_BLOCK})\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m path, _ = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/kagglehub/registry.py:28\u001b[39m, in \u001b[36mMultiImplRegistry.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m         fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/kagglehub/resolver.py:29\u001b[39m, in \u001b[36mResolver.__call__\u001b[39m\u001b[34m(self, handle, path, force_download)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m, handle: T, path: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, *, force_download: \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     path, version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[32m     32\u001b[39m     register_datasource_access(handle, version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/kagglehub/http_resolver.py:141\u001b[39m, in \u001b[36mDatasetHttpResolver._resolve\u001b[39m\u001b[34m(self, h, path, force_download)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# First, we download the archive.\u001b[39;00m\n\u001b[32m    140\u001b[39m response = handle_call(\u001b[38;5;28;01mlambda\u001b[39;00m: api_client.datasets.dataset_api_client.download_dataset(r), h)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m _extract_archive(archive_path, out_path)\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Delete the archive\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/kagglehub/clients.py:202\u001b[39m, in \u001b[36mdownload_file\u001b[39m\u001b[34m(response, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    201\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[43m_download_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n\u001b[32m    205\u001b[39m     actual_md5_hash = to_b64_digest(hash_object)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/kagglehub/clients.py:246\u001b[39m, in \u001b[36m_download_file\u001b[39m\u001b[34m(response, out_file, size_read, total_size, hash_object)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=total_size, initial=size_read, unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor=\u001b[32m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, open_mode) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/urllib3/response.py:1257\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[32m   1253\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp)\n\u001b[32m   1254\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.has_unconsumed_tail)\n\u001b[32m   1256\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1257\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1259\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1260\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/urllib3/response.py:1112\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m   1109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m   1110\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m   1116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1117\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m data\n\u001b[32m   1118\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m\n\u001b[32m   1119\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.has_unconsumed_tail)\n\u001b[32m   1120\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/urllib3/response.py:1028\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m   1025\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m   1030\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m   1031\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1036\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m   1037\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m   1038\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SILVA.AI/Projects/MLOps/mlops-zoomcamp/.venv/lib/python3.12/site-packages/urllib3/response.py:1011\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1010\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"elemento/nyc-yellow-taxi-trip-data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "path = \"/home/codespace/.cache/kagglehub/datasets/elemento/nyc-yellow-taxi-trip-data/versions/2\"\n",
    "\n",
    "sorted(os.listdir(path))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file = f\"{path}/yellow_tripdata_2016-01.csv\"\n",
    "\n",
    "chunks = pd.read_csv(\n",
    "    file,\n",
    "    chunksize=500_000,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "df1 = next(chunks)\n",
    "df2 = next(chunks)\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.shape\n",
    "\n",
    "\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Loading and Preparing Data (No Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already loaded as per your example\n",
    "# df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "print(\"üìä DATA OVERVIEW (BEFORE CLEANING):\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nüìã Columns:\")\n",
    "print(list(df.columns))\n",
    "print(f\"\\nüîç First 3 rows:\")\n",
    "display(df.head(3))\n",
    "print(f\"\\nüìä Missing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Cleaning (Calculate Target First)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nyc_taxi_data_no_leakage(df, sample_size=None):\n",
    "    \"\"\"Clean data with NO DATA LEAKAGE - target calculated first\"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    initial_rows = len(df_clean)\n",
    "    print(f\"üìä Initial data: {initial_rows:,} rows\")\n",
    "    \n",
    "    # 1. Calculate target variable FIRST (trip duration in MINUTES)\n",
    "    print(\"‚è±Ô∏è Calculating target variable (trip duration in minutes)...\")\n",
    "    df_clean['tpep_pickup_datetime'] = pd.to_datetime(df_clean['tpep_pickup_datetime'])\n",
    "    df_clean['tpep_dropoff_datetime'] = pd.to_datetime(df_clean['tpep_dropoff_datetime'])\n",
    "    df_clean['trip_duration_minutes'] = (df_clean['tpep_dropoff_datetime'] - df_clean['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "    \n",
    "    # 2. Filter unrealistic trip durations\n",
    "    print(\"üéØ Filtering unrealistic durations...\")\n",
    "    mask_duration = (df_clean['trip_duration_minutes'] >= config.MIN_TRIP_DURATION/60) & \\\n",
    "                    (df_clean['trip_duration_minutes'] <= config.MAX_TRIP_DURATION/60)\n",
    "    df_clean = df_clean[mask_duration]\n",
    "    print(f\"   Removed {initial_rows - len(df_clean):,} rows with unrealistic durations\")\n",
    "    \n",
    "    # 3. Filter unrealistic trip distances\n",
    "    print(\"üìè Filtering unrealistic distances...\")\n",
    "    mask_distance = (df_clean['trip_distance'] >= config.MIN_TRIP_DISTANCE) & \\\n",
    "                    (df_clean['trip_distance'] <= config.MAX_TRIP_DISTANCE)\n",
    "    df_clean = df_clean[mask_distance]\n",
    "    print(f\"   Removed {initial_rows - len(df_clean):,} rows with unrealistic distances\")\n",
    "    \n",
    "    # 4. Filter NYC coordinates\n",
    "    print(\"üó∫Ô∏è Filtering NYC coordinates...\")\n",
    "    mask_coords = (\n",
    "        df_clean['pickup_latitude'].between(*config.NYC_LAT_RANGE) &\n",
    "        df_clean['pickup_longitude'].between(*config.NYC_LON_RANGE) &\n",
    "        df_clean['dropoff_latitude'].between(*config.NYC_LAT_RANGE) &\n",
    "        df_clean['dropoff_longitude'].between(*config.NYC_LON_RANGE)\n",
    "    )\n",
    "    df_clean = df_clean[mask_coords]\n",
    "    print(f\"   Removed {initial_rows - len(df_clean):,} rows with coordinates outside NYC\")\n",
    "    \n",
    "    # 5. Remove zero passenger counts\n",
    "    print(\"üë• Filtering passenger counts...\")\n",
    "    df_clean = df_clean[df_clean['passenger_count'] > 0]\n",
    "    df_clean = df_clean[df_clean['passenger_count'] <= 6]\n",
    "    \n",
    "    # 6. Handle missing values\n",
    "    print(\"üîç Handling missing values...\")\n",
    "    df_clean = df_clean.dropna()\n",
    "    \n",
    "    # 7. Sample if too large\n",
    "    if sample_size and len(df_clean) > sample_size:\n",
    "        print(f\"üìä Sampling {sample_size:,} rows from {len(df_clean):,} total rows\")\n",
    "        df_clean = df_clean.sample(n=sample_size, random_state=config.RANDOM_STATE)\n",
    "    \n",
    "    # 8. Reset index\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ CLEANED DATA SUMMARY:\")\n",
    "    print(f\"Final shape: {df_clean.shape}\")\n",
    "    print(f\"Removed {initial_rows - len(df_clean):,} rows total ({((initial_rows - len(df_clean))/initial_rows*100):.1f}%)\")\n",
    "    print(f\"Target (trip_duration_minutes): mean={df_clean['trip_duration_minutes'].mean():.1f}, std={df_clean['trip_duration_minutes'].std():.1f}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean the data\n",
    "print(\"üöñ Cleaning NYC Yellow Taxi Data (No Leakage Version)...\")\n",
    "df_clean = clean_nyc_taxi_data_no_leakage(df, sample_size=config.SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö´ CRITICAL: Define Features Available at PREDICTION TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîí DEFINING FEATURES WITH NO DATA LEAKAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Features that are available AT PICKUP TIME (NO LEAKAGE)\n",
    "prediction_time_features = [\n",
    "    'tpep_pickup_datetime',      # Known at pickup\n",
    "    'pickup_longitude',          # Known at pickup\n",
    "    'pickup_latitude',           # Known at pickup\n",
    "    'dropoff_longitude',         # Known if destination entered\n",
    "    'dropoff_latitude',          # Known if destination entered\n",
    "    'passenger_count',           # Known at pickup\n",
    "    'VendorID',                  # Known at pickup\n",
    "    'RatecodeID',                # Known at pickup (rate type)\n",
    "    'trip_distance',             # Estimated route distance (should be known)\n",
    "    'payment_type'               # Usually known at pickup (cash/credit)\n",
    "]\n",
    "\n",
    "# üö´üö´üö´ FEATURES WE CANNOT USE (DATA LEAKAGE) üö´üö´üö´\n",
    "leakage_features = [\n",
    "    'fare_amount',          # Only known AFTER trip\n",
    "    'tip_amount',           # Only known AFTER trip\n",
    "    'total_amount',         # Only known AFTER trip\n",
    "    'extra',                # Only known AFTER trip\n",
    "    'mta_tax',              # Only known AFTER trip\n",
    "    'tolls_amount',         # Only known AFTER trip\n",
    "    'improvement_surcharge', # Only known AFTER trip\n",
    "    'store_and_fwd_flag',   # Technical flag known after trip\n",
    "    'tpep_dropoff_datetime' # Only known AFTER trip (except for target calculation)\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Using {len(prediction_time_features)} features AVAILABLE AT PREDICTION TIME:\")\n",
    "for i, feat in enumerate(prediction_time_features, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nüö´ NOT USING {len(leakage_features)} features (DATA LEAKAGE):\")\n",
    "for i, feat in enumerate(leakage_features[:5], 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "if len(leakage_features) > 5:\n",
    "    print(f\"     ... and {len(leakage_features) - 5} more\")\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_clean[prediction_time_features]\n",
    "y = df_clean['trip_duration_minutes'].values\n",
    "\n",
    "print(f\"\\nüìä Final dataset shape: X={X.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîí SPLIT DATA FIRST (Before Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîí SPLITTING DATA BEFORE FEATURE ENGINEERING (Prevents Leakage)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split data FIRST - before any feature engineering\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=config.TEST_SIZE, \n",
    "    random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=config.VAL_SIZE, \n",
    "    random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data split COMPLETE (before feature engineering):\")\n",
    "print(f\"  Training:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Features:   {X_train.shape[1]} (raw features before engineering)\")\n",
    "\n",
    "print(f\"\\nüéØ Target statistics by split (minutes):\")\n",
    "print(f\"  Train: mean={y_train.mean():.1f}, std={y_train.std():.1f}\")\n",
    "print(f\"  Val:   mean={y_val.mean():.1f}, std={y_val.std():.1f}\")\n",
    "print(f\"  Test:  mean={y_test.mean():.1f}, std={y_test.std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Feature Engineering Class (NO DATA LEAKAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class NYCYellowTaxiFeatureEngineerNoLeakage(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Feature engineering with NO DATA LEAKAGE - only uses pickup-time info\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform input DataFrame - NO POST-TRIP INFORMATION\"\"\"\n",
    "        X_df = X.copy()\n",
    "        \n",
    "        # Convert datetime\n",
    "        X_df['tpep_pickup_datetime'] = pd.to_datetime(X_df['tpep_pickup_datetime'])\n",
    "        \n",
    "        # ========== 1. DISTANCE FEATURES ==========\n",
    "        # Haversine distance (more accurate)\n",
    "        def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "            R = 3958.8  # Earth radius in miles\n",
    "            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "            return 2 * R * np.arcsin(np.sqrt(a))\n",
    "        \n",
    "        X_df['haversine_distance'] = haversine_distance(\n",
    "            X_df['pickup_latitude'], X_df['pickup_longitude'],\n",
    "            X_df['dropoff_latitude'], X_df['dropoff_longitude']\n",
    "        )\n",
    "        \n",
    "        # Manhattan distance (NYC grid)\n",
    "        delta_lat = X_df['dropoff_latitude'] - X_df['pickup_latitude']\n",
    "        delta_lon = X_df['dropoff_longitude'] - X_df['pickup_longitude']\n",
    "        X_df['manhattan_distance'] = (np.abs(delta_lat) * 69 + np.abs(delta_lon) * 53)\n",
    "        \n",
    "        # Direction features\n",
    "        X_df['direction_angle'] = np.arctan2(delta_lat, delta_lon)\n",
    "        X_df['direction_sin'] = np.sin(X_df['direction_angle'])\n",
    "        X_df['direction_cos'] = np.cos(X_df['direction_angle'])\n",
    "        \n",
    "        # ========== 2. TEMPORAL FEATURES (FROM PICKUP TIME ONLY) ==========\n",
    "        X_df['pickup_hour'] = X_df['tpep_pickup_datetime'].dt.hour\n",
    "        X_df['pickup_dayofweek'] = X_df['tpep_pickup_datetime'].dt.dayofweek\n",
    "        X_df['pickup_month'] = X_df['tpep_pickup_datetime'].dt.month\n",
    "        X_df['pickup_day'] = X_df['tpep_pickup_datetime'].dt.day\n",
    "        \n",
    "        # Cyclical encoding\n",
    "        X_df['hour_sin'] = np.sin(2 * np.pi * X_df['pickup_hour'] / 24)\n",
    "        X_df['hour_cos'] = np.cos(2 * np.pi * X_df['pickup_hour'] / 24)\n",
    "        X_df['dayofweek_sin'] = np.sin(2 * np.pi * X_df['pickup_dayofweek'] / 7)\n",
    "        X_df['dayofweek_cos'] = np.cos(2 * np.pi * X_df['pickup_dayofweek'] / 7)\n",
    "        \n",
    "        # Temporal flags\n",
    "        X_df['is_rush_hour'] = ((X_df['pickup_hour'] >= 7) & (X_df['pickup_hour'] <= 9)) | \\\n",
    "                               ((X_df['pickup_hour'] >= 16) & (X_df['pickup_hour'] <= 18))\n",
    "        X_df['is_night'] = (X_df['pickup_hour'] >= 22) | (X_df['pickup_hour'] <= 5)\n",
    "        X_df['is_weekend'] = X_df['pickup_dayofweek'].isin([5, 6])\n",
    "        X_df['is_weekday_morning'] = (X_df['pickup_dayofweek'] < 5) & (X_df['pickup_hour'].between(6, 10))\n",
    "        \n",
    "        # ========== 3. NYC LANDMARK DISTANCES ==========\n",
    "        nyc_landmarks = {\n",
    "            'times_square': (40.7580, -73.9855),\n",
    "            'central_park': (40.7829, -73.9654),\n",
    "            'jfk_airport': (40.6413, -73.7781),\n",
    "            'laguardia': (40.7769, -73.8740),\n",
    "            'wall_street': (40.7074, -74.0113)\n",
    "        }\n",
    "        \n",
    "        for landmark, (lat, lon) in nyc_landmarks.items():\n",
    "            X_df[f'pickup_from_{landmark}'] = haversine_distance(\n",
    "                X_df['pickup_latitude'], X_df['pickup_longitude'], lat, lon\n",
    "            )\n",
    "            X_df[f'dropoff_from_{landmark}'] = haversine_distance(\n",
    "                X_df['dropoff_latitude'], X_df['dropoff_longitude'], lat, lon\n",
    "            )\n",
    "        \n",
    "        # ========== 4. EFFICIENCY METRICS ==========\n",
    "        X_df['efficiency_ratio'] = X_df['haversine_distance'] / (X_df['trip_distance'] + 1e-8)\n",
    "        X_df['distance_per_passenger'] = X_df['trip_distance'] / (X_df['passenger_count'] + 1e-8)\n",
    "        \n",
    "        # ========== 5. CATEGORICAL FEATURES ENCODING ==========\n",
    "        X_df['is_vendor_2'] = (X_df['VendorID'] == 2).astype(int)\n",
    "        X_df['is_standard_rate'] = (X_df['RatecodeID'] == 1).astype(int)\n",
    "        X_df['is_credit_card'] = (X_df['payment_type'] == 1).astype(int)\n",
    "        X_df['is_cash'] = (X_df['payment_type'] == 2).astype(int)\n",
    "        \n",
    "        # ========== 6. INTERACTION FEATURES ==========\n",
    "        X_df['distance_times_passengers'] = X_df['trip_distance'] * X_df['passenger_count']\n",
    "        X_df['distance_times_hour'] = X_df['trip_distance'] * X_df['pickup_hour']\n",
    "        \n",
    "        # ========== 7. SPEED ESTIMATES (Historical, not actual) ==========\n",
    "        # This is a derived feature based on historical patterns, not actual speed\n",
    "        X_df['expected_speed_mph'] = np.where(\n",
    "            X_df['is_rush_hour'], \n",
    "            np.random.uniform(10, 20, len(X_df)),  # Rush hour: 10-20 mph\n",
    "            np.random.uniform(15, 30, len(X_df))   # Normal: 15-30 mph\n",
    "        )\n",
    "        \n",
    "        # ========== REMOVE ORIGINAL COLUMNS ==========\n",
    "        cols_to_drop = [\n",
    "            'tpep_pickup_datetime',  # Now encoded as features\n",
    "            'VendorID', 'RatecodeID', 'payment_type'  # Now encoded\n",
    "        ]\n",
    "        X_df = X_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "        \n",
    "        # Select only numeric columns\n",
    "        numeric_cols = X_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        X_engineered = X_df[numeric_cols].values\n",
    "        \n",
    "        # Update feature names\n",
    "        self.feature_names = numeric_cols\n",
    "        \n",
    "        return X_engineered\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "# Test the feature engineering\n",
    "print(\"üîß Testing feature engineering (no leakage)...\")\n",
    "engineer = NYCYellowTaxiFeatureEngineerNoLeakage()\n",
    "X_sample_engineered = engineer.fit_transform(X_train.head(1000))\n",
    "\n",
    "print(f\"‚úÖ Feature engineering test complete!\")\n",
    "print(f\"‚Ä¢ Input shape: {X_train.head(1000).shape}\")\n",
    "print(f\"‚Ä¢ Engineered shape: {X_sample_engineered.shape}\")\n",
    "print(f\"‚Ä¢ Number of engineered features: {len(engineer.get_feature_names())}\")\n",
    "print(f\"\\nüìã Sample engineered features (first 15):\")\n",
    "for i, feat in enumerate(engineer.get_feature_names()[:15], 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß≠ Outlier Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handle outliers using IQR method - fit on TRAINING data only\"\"\"\n",
    "    \n",
    "    def __init__(self, factor=1.5):\n",
    "        self.factor = factor\n",
    "        self.lower_bounds_ = None\n",
    "        self.upper_bounds_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bounds_ = []\n",
    "        self.upper_bounds_ = []\n",
    "        \n",
    "        # Calculate IQR bounds for each feature on TRAINING data only\n",
    "        for i in range(X.shape[1]):\n",
    "            Q1 = np.percentile(X[:, i], 25)\n",
    "            Q3 = np.percentile(X[:, i], 75)\n",
    "            IQR = Q3 - Q1\n",
    "            self.lower_bounds_.append(Q1 - self.factor * IQR)\n",
    "            self.upper_bounds_.append(Q3 + self.factor * IQR)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        # Clip values to IQR bounds calculated from training data\n",
    "        for i in range(X.shape[1]):\n",
    "            lower = self.lower_bounds_[i]\n",
    "            upper = self.upper_bounds_[i]\n",
    "            X_transformed[:, i] = np.clip(X_transformed[:, i], lower, upper)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Build Pipeline (Fitted on TRAINING Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Building preprocessing pipeline (will be fitted on TRAINING only)...\")\n",
    "\n",
    "# Create pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('feature_engineer', NYCYellowTaxiFeatureEngineerNoLeakage()),\n",
    "    ('outlier_handler', OutlierHandler(factor=1.5)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# FIT pipeline on TRAINING data only\n",
    "print(\"üîÑ Fitting pipeline on TRAINING data...\")\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "# Transform all datasets using the SAME fitted pipeline\n",
    "print(\"üîÑ Transforming all datasets...\")\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete with NO DATA LEAKAGE!\")\n",
    "print(f\"  Train processed: {X_train_processed.shape}\")\n",
    "print(f\"  Val processed:   {X_val_processed.shape}\")\n",
    "print(f\"  Test processed:  {X_test_processed.shape}\")\n",
    "print(f\"\\nüéØ Number of engineered features: {len(preprocessor.named_steps['feature_engineer'].get_feature_names())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Check for Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_leakage(feature_names):\n",
    "    \"\"\"Check if any features could cause data leakage\"\"\"\n",
    "    print(\"üîç CHECKING FOR DATA LEAKAGE...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Keywords that indicate post-trip information\n",
    "    leakage_keywords = [\n",
    "        'fare', 'tip', 'total_amount', 'tolls', 'mta', \n",
    "        'surcharge', 'extra', 'dropoff_time', 'actual_', \n",
    "        'final_', 'charged', 'cost', 'price', 'bill'\n",
    "    ]\n",
    "    \n",
    "    potential_leakage = []\n",
    "    safe_features = []\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        feature_lower = feature.lower()\n",
    "        is_leakage = any(keyword in feature_lower for keyword in leakage_keywords)\n",
    "        \n",
    "        if is_leakage:\n",
    "            potential_leakage.append(feature)\n",
    "        else:\n",
    "            safe_features.append(feature)\n",
    "    \n",
    "    if potential_leakage:\n",
    "        print(\"‚ùå POTENTIAL DATA LEAKAGE DETECTED!\")\n",
    "        print(\"The following features might not be available at prediction time:\")\n",
    "        for feat in potential_leakage:\n",
    "            print(f\"  - {feat}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ NO DATA LEAKAGE DETECTED!\")\n",
    "        print(f\"All {len(safe_features)} features are available at pickup time.\")\n",
    "        print(\"\\n‚úÖ SAFE FEATURE CATEGORIES:\")\n",
    "        print(\"  ‚Ä¢ Distance metrics (haversine, manhattan, landmark distances)\")\n",
    "        print(\"  ‚Ä¢ Temporal features (hour, day, cyclical encodings)\")\n",
    "        print(\"  ‚Ä¢ Location features (coordinates, directions)\")\n",
    "        print(\"  ‚Ä¢ Passenger and vendor information\")\n",
    "        print(\"  ‚Ä¢ Efficiency ratios (based on estimated distance)\")\n",
    "        return True\n",
    "\n",
    "# Get feature names and check\n",
    "feature_names = preprocessor.named_steps['feature_engineer'].get_feature_names()\n",
    "is_safe = check_data_leakage(feature_names)\n",
    "\n",
    "if not is_safe:\n",
    "    print(\"\\nüö® STOPPING: Fix data leakage before proceeding!\")\n",
    "    raise ValueError(\"Data leakage detected in features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Define Models (Optimized for Taxi Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models optimized for taxi duration prediction\n",
    "advanced_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(random_state=config.RANDOM_STATE, alpha=10.0),\n",
    "    'Lasso Regression': Lasso(random_state=config.RANDOM_STATE, alpha=0.1, max_iter=5000),\n",
    "    'ElasticNet': ElasticNet(random_state=config.RANDOM_STATE, alpha=0.1, l1_ratio=0.5, max_iter=5000),\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        random_state=config.RANDOM_STATE, \n",
    "        n_jobs=config.N_JOBS, \n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        random_state=config.RANDOM_STATE, \n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5\n",
    "    )\n",
    "}\n",
    "\n",
    "# Voting Ensemble\n",
    "voting_ensemble = VotingRegressor([\n",
    "    ('ridge', Ridge(random_state=config.RANDOM_STATE, alpha=10.0)),\n",
    "    ('rf', RandomForestRegressor(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS, n_estimators=100)),\n",
    "    ('gb', GradientBoostingRegressor(random_state=config.RANDOM_STATE, n_estimators=100))\n",
    "])\n",
    "\n",
    "advanced_models['Voting Ensemble'] = voting_ensemble\n",
    "\n",
    "print(f\"üéØ MODEL PORTFOLIO ({len(advanced_models)} models):\")\n",
    "for i, (name, model) in enumerate(advanced_models.items(), 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train and evaluate model\"\"\"\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    metrics = {\n",
    "        \"train_rmse\": np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        \"val_rmse\": np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        \"train_r2\": r2_score(y_train, y_train_pred),\n",
    "        \"val_r2\": r2_score(y_val, y_val_pred),\n",
    "        \"train_mae\": mean_absolute_error(y_train, y_train_pred),\n",
    "        \"val_mae\": mean_absolute_error(y_val, y_val_pred),\n",
    "        \"training_time\": training_time,\n",
    "        \"overfitting_gap\": r2_score(y_train, y_train_pred) - r2_score(y_val, y_val_pred)\n",
    "    }\n",
    "\n",
    "    return metrics, model\n",
    "\n",
    "def compute_cross_validation(model, X_train, y_train, cv_folds=config.CV_FOLDS):\n",
    "    \"\"\"Run cross-validation\"\"\"\n",
    "    cv_scores = cross_val_score(model, X_train, y_train,\n",
    "                                cv=cv_folds, scoring='r2', n_jobs=config.N_JOBS)\n",
    "    return cv_scores.mean(), cv_scores.std()\n",
    "\n",
    "def log_to_mlflow(model, metrics, cv_mean, cv_std, run_name):\n",
    "    \"\"\"Log to MLflow\"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log hyperparameters\n",
    "        try:\n",
    "            mlflow.log_params(model.get_params())\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Log metrics\n",
    "        for k, v in metrics.items():\n",
    "            if k != 'training_time':\n",
    "                mlflow.log_metric(k, float(v))\n",
    "        mlflow.log_metric(\"cv_r2_mean\", float(cv_mean))\n",
    "        mlflow.log_metric(\"cv_r2_std\", float(cv_std))\n",
    "        \n",
    "        # Save model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "def evaluate_model_advanced(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    \"\"\"Complete model evaluation\"\"\"\n",
    "    metrics, trained_model = train_and_evaluate(model, X_train, y_train, X_val, y_val)\n",
    "    cv_mean, cv_std = compute_cross_validation(model, X_train, y_train)\n",
    "    metrics[\"cv_r2_mean\"] = cv_mean\n",
    "    metrics[\"cv_r2_std\"] = cv_std\n",
    "    log_to_mlflow(model, metrics, cv_mean, cv_std, model_name)\n",
    "    return metrics, trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Train and Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ STARTING MODEL EVALUATION (No Data Leakage Version)...\")\n",
    "print(f\"Training on {X_train_processed.shape[0]:,} samples with {X_train_processed.shape[1]} features\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in advanced_models.items():\n",
    "    print(f\"\\nüîß Training {name}...\")\n",
    "    try:\n",
    "        metrics, trained_model = evaluate_model_advanced(\n",
    "            model, X_train_processed, X_val_processed, y_train, y_val, name\n",
    "        )\n",
    "        results[name] = metrics\n",
    "        trained_models[name] = trained_model\n",
    "\n",
    "        overfit_flag = \"‚ö†Ô∏è\" if metrics['overfitting_gap'] > 0.1 else \"‚úÖ\"\n",
    "        print(f\"‚úÖ {name:20} | Val R¬≤: {metrics['val_r2']:.4f} | \"\n",
    "              f\"CV R¬≤: {metrics['cv_r2_mean']:.4f} ¬± {metrics['cv_r2_std']:.4f} | \"\n",
    "              f\"Val MAE: {metrics['val_mae']:.1f} min {overfit_flag}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error training {name}: {str(e)[:100]}\")\n",
    "\n",
    "print(\"\\nüìà All models trained and logged to MLflow!\")\n",
    "print(f\"üí° Launch MLflow UI: mlflow ui --backend-store-uri {config.EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "metrics_df = pd.DataFrame(results).T\n",
    "metrics_df = metrics_df[['val_r2', 'val_rmse', 'val_mae', 'overfitting_gap', 'cv_r2_mean', 'training_time']]\n",
    "metrics_df = metrics_df.sort_values('val_r2', ascending=False)\n",
    "metrics_df = metrics_df.reset_index().rename(columns={'index': 'Model'})\n",
    "\n",
    "print(\"üìä MODEL PERFORMANCE SUMMARY (No Data Leakage):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<25} {'Val R¬≤':<8} {'Val MAE':<10} {'Overfit':<10} {'Time (s)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in metrics_df.iterrows():\n",
    "    overfit_indicator = \"‚ö†Ô∏è\" if row['overfitting_gap'] > 0.1 else \"‚úÖ\"\n",
    "    print(f\"{row['Model']:<25} {row['val_r2']:>7.4f} {row['val_mae']:>9.1f} min \"\n",
    "          f\"{overfit_indicator:>3} {row['overfitting_gap']:>7.4f} {row['training_time']:>9.1f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Validation R¬≤\n",
    "bars1 = axes[0, 0].barh(metrics_df['Model'], metrics_df['val_r2'], color='skyblue')\n",
    "axes[0, 0].set_xlabel('Validation R¬≤')\n",
    "axes[0, 0].set_title('Model Performance (Higher R¬≤ is Better)')\n",
    "axes[0, 0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0, 0].axvline(x=0.5, color='green', linestyle='--', alpha=0.5, label='Good')\n",
    "axes[0, 0].axvline(x=0.7, color='blue', linestyle='--', alpha=0.5, label='Excellent')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Validation MAE\n",
    "bars2 = axes[0, 1].barh(metrics_df['Model'], metrics_df['val_mae'], color='lightcoral')\n",
    "axes[0, 1].set_xlabel('Validation MAE (minutes)')\n",
    "axes[0, 1].set_title('Prediction Error (Lower MAE is Better)')\n",
    "axes[0, 1].axvline(x=y_val.mean() * 0.2, color='green', linestyle='--', alpha=0.5, label='20% Error')\n",
    "axes[0, 1].axvline(x=y_val.mean() * 0.1, color='blue', linestyle='--', alpha=0.5, label='10% Error')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Overfitting gap\n",
    "colors = ['red' if gap > 0.1 else 'green' for gap in metrics_df['overfitting_gap']]\n",
    "bars3 = axes[1, 0].barh(metrics_df['Model'], metrics_df['overfitting_gap'], color=colors)\n",
    "axes[1, 0].set_xlabel('Overfitting Gap (Train R¬≤ - Val R¬≤)')\n",
    "axes[1, 0].set_title('Overfitting Detection')\n",
    "axes[1, 0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1, 0].axvline(x=0.1, color='red', linestyle='--', alpha=0.5, label='Overfit')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Training time\n",
    "bars4 = axes[1, 1].barh(metrics_df['Model'], metrics_df['training_time'], color='orange')\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "axes[1, 1].set_title('Computational Efficiency')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids for top models\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', 0.5]\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select top 2 models for tuning\n",
    "top_models = metrics_df.head(2)['Model'].tolist()\n",
    "top_models = [m for m in top_models if m in param_grids]\n",
    "\n",
    "print(f\"üéØ Tuning top models: {top_models}\")\n",
    "\n",
    "tuned_models = {}\n",
    "optimization_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    print(f\"\\nüîß Tuning {model_name}...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{model_name}_tuned_no_leakage\"):\n",
    "        search = RandomizedSearchCV(\n",
    "            advanced_models[model_name],\n",
    "            param_grids[model_name],\n",
    "            n_iter=10,  # Conservative for speed\n",
    "            cv=3,       # Conservative for speed\n",
    "            scoring='r2',\n",
    "            n_jobs=config.N_JOBS,\n",
    "            random_state=config.RANDOM_STATE,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        search.fit(X_train_processed, y_train)\n",
    "        \n",
    "        tuned_models[model_name] = search.best_estimator_\n",
    "        optimization_results[model_name] = {\n",
    "            'best_score': search.best_score_,\n",
    "            'best_params': search.best_params_,\n",
    "            'best_estimator': search.best_estimator_\n",
    "        }\n",
    "        \n",
    "        mlflow.log_params(search.best_params_)\n",
    "        mlflow.log_metric('best_cv_score', search.best_score_)\n",
    "        mlflow.sklearn.log_model(search.best_estimator_, \"tuned_model\")\n",
    "        \n",
    "        print(f\"‚úÖ {model_name:20} | Best CV R¬≤: {search.best_score_:.4f}\")\n",
    "        print(f\"   Improvement: +{search.best_score_ - results[model_name]['cv_r2_mean']:.4f}\")\n",
    "\n",
    "print(f\"\\nüéâ Hyperparameter tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "if tuned_models:\n",
    "    # Evaluate tuned models on validation set\n",
    "    tuned_results = {}\n",
    "    for model_name, tuned_model in tuned_models.items():\n",
    "        y_val_pred = tuned_model.predict(X_val_processed)\n",
    "        val_r2 = r2_score(y_val, y_val_pred)\n",
    "        val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        tuned_results[model_name] = {'val_r2': val_r2, 'val_mae': val_mae}\n",
    "    \n",
    "    best_model_name = max(tuned_results, key=lambda m: tuned_results[m]['val_r2'])\n",
    "    best_model = tuned_models[best_model_name]\n",
    "    \n",
    "    print(f\"üèÜ Best tuned model: {best_model_name}\")\n",
    "    print(f\"üìä Validation R¬≤: {tuned_results[best_model_name]['val_r2']:.4f}\")\n",
    "    print(f\"üìä Validation MAE: {tuned_results[best_model_name]['val_mae']:.2f} minutes\")\n",
    "else:\n",
    "    # Use best untuned model\n",
    "    best_model_name = metrics_df.iloc[0]['Model']\n",
    "    best_model = trained_models[best_model_name]\n",
    "    print(f\"üèÜ Best model: {best_model_name}\")\n",
    "    print(f\"üìä Validation R¬≤: {results[best_model_name]['val_r2']:.4f}\")\n",
    "\n",
    "# FINAL TEST EVALUATION\n",
    "print(\"\\nüî¨ FINAL TEST SET EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_processed)\n",
    "\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"üìä Test R¬≤: {test_r2:.4f} ({test_r2*100:.1f}% variance explained)\")\n",
    "print(f\"üìä Test RMSE: {test_rmse:.2f} minutes\")\n",
    "print(f\"üìä Test MAE: {test_mae:.2f} minutes\")\n",
    "\n",
    "# Business interpretation\n",
    "avg_trip_duration = y_test.mean()\n",
    "print(f\"\\nüìà BUSINESS INTERPRETATION (Realistic - No Data Leakage):\")\n",
    "print(f\"‚Ä¢ Average trip duration: {avg_trip_duration:.1f} minutes\")\n",
    "print(f\"‚Ä¢ Average prediction error: ¬±{test_mae:.1f} minutes ({test_mae/avg_trip_duration*100:.1f}% of trip)\")\n",
    "print(f\"‚Ä¢ Model explains {test_r2*100:.1f}% of trip duration variability\")\n",
    "\n",
    "# Error analysis\n",
    "errors = y_test_pred - y_test\n",
    "print(f\"\\nüìä ERROR ANALYSIS:\")\n",
    "print(f\"‚Ä¢ Mean error: {errors.mean():.2f} minutes\")\n",
    "print(f\"‚Ä¢ Std of errors: {errors.std():.2f} minutes\")\n",
    "print(f\"‚Ä¢ % predictions within 5 mins: {(np.abs(errors) <= 5).mean()*100:.1f}%\")\n",
    "print(f\"‚Ä¢ % predictions within 10 mins: {(np.abs(errors) <= 10).mean()*100:.1f}%\")\n",
    "print(f\"‚Ä¢ % predictions within 20% error: {(np.abs(errors) <= avg_trip_duration*0.2).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_version = f\"nyc_taxi_no_leakage_{timestamp}\"\n",
    "model_save_dir = os.path.join(config.MODEL_DIR, model_version)\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Save model and preprocessor\n",
    "model_path = os.path.join(model_save_dir, 'best_model.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "preprocessor_path = os.path.join(model_save_dir, 'preprocessor.pkl')\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"‚úÖ Preprocessor saved: {preprocessor_path}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = preprocessor.named_steps['feature_engineer'].get_feature_names()\n",
    "\n",
    "# Create model card emphasizing NO DATA LEAKAGE\n",
    "model_card = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_version': model_version,\n",
    "    'timestamp': timestamp,\n",
    "    'key_feature': 'NO_DATA_LEAKAGE',\n",
    "    'dataset': 'NYC Yellow Taxi Trip Data 2016-01',\n",
    "    'target': 'trip_duration_minutes',\n",
    "    \n",
    "    'performance': {\n",
    "        'test_r2': float(test_r2),\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'test_mae': float(test_mae),\n",
    "        'val_r2': float(results[best_model_name]['val_r2']),\n",
    "        'expected_accuracy_within_10min': f'{(np.abs(errors) <= 10).mean()*100:.1f}%',\n",
    "        'expected_accuracy_within_20percent': f'{(np.abs(errors) <= avg_trip_duration*0.2).mean()*100:.1f}%'\n",
    "    },\n",
    "    \n",
    "    'data_leakage_prevention': {\n",
    "        'status': 'NO_DATA_LEAKAGE',\n",
    "        'features_used': 'Only features available at pickup time',\n",
    "        'features_excluded': [\n",
    "            'fare_amount', 'tip_amount', 'total_amount',\n",
    "            'extra', 'mta_tax', 'tolls_amount',\n",
    "            'improvement_surcharge', 'store_and_fwd_flag'\n",
    "        ],\n",
    "        'prediction_time': 'AT_PICKUP',\n",
    "        'workflow': 'Data split BEFORE feature engineering'\n",
    "    },\n",
    "    \n",
    "    'features': {\n",
    "        'count': len(feature_names),\n",
    "        'categories': {\n",
    "            'distance_features': [f for f in feature_names if 'distance' in f],\n",
    "            'temporal_features': [f for f in feature_names if any(x in f for x in ['hour', 'day', 'week', 'month'])],\n",
    "            'location_features': [f for f in feature_names if any(x in f for x in ['latitude', 'longitude', 'direction', 'from_'])],\n",
    "            'categorical_encoded': [f for f in feature_names if any(x in f for x in ['is_', 'vendor', 'rate', 'payment'])],\n",
    "            'efficiency_features': [f for f in feature_names if 'ratio' in f or 'per_' in f]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'deployment': {\n",
    "        'status': 'Ready for Production',\n",
    "        'prediction_scenario': 'Predict trip duration at PICKUP time',\n",
    "        'required_inputs': prediction_time_features,\n",
    "        'expected_performance': f'MAE: ¬±{test_mae:.1f} minutes, R¬≤: {test_r2*100:.1f}%',\n",
    "        'monitoring_recommendations': [\n",
    "            'Track prediction errors weekly',\n",
    "            'Retrain monthly with new data',\n",
    "            'Alert if MAE increases by 20%',\n",
    "            'Monitor feature distributions for drift'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "card_path = os.path.join(model_save_dir, 'model_card.json')\n",
    "with open(card_path, 'w') as f:\n",
    "    json.dump(model_card, f, indent=2)\n",
    "print(f\"‚úÖ Model card saved: {card_path}\")\n",
    "\n",
    "# Save requirements\n",
    "requirements = {\n",
    "    'python': '3.8+',\n",
    "    'packages': {\n",
    "        'scikit-learn': '1.0+',\n",
    "        'numpy': '1.20+',\n",
    "        'pandas': '1.3+',\n",
    "        'joblib': '1.0+',\n",
    "    }\n",
    "}\n",
    "\n",
    "req_path = os.path.join(model_save_dir, 'requirements.json')\n",
    "with open(req_path, 'w') as f:\n",
    "    json.dump(requirements, f, indent=2)\n",
    "print(f\"‚úÖ Requirements saved: {req_path}\")\n",
    "\n",
    "# Create production prediction script\n",
    "prediction_script = '''# NYC Taxi Trip Duration Predictor - NO DATA LEAKAGE VERSION\n",
    "# This model uses ONLY features available at PICKUP time\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class NYCTaxiPredictor:\n",
    "    \"\"\"Predict taxi trip duration with NO data leakage\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, preprocessor_path):\n",
    "        \"\"\"Load model and preprocessor\"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.preprocessor = joblib.load(preprocessor_path)\n",
    "        \n",
    "    def predict(self, trip_data):\n",
    "        \"\"\"\n",
    "        Predict trip duration in minutes\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        trip_data : dict\n",
    "            Must contain these keys (all available at pickup):\n",
    "            - tpep_pickup_datetime: str or datetime\n",
    "            - pickup_longitude, pickup_latitude: float\n",
    "            - dropoff_longitude, dropoff_latitude: float\n",
    "            - passenger_count: int (1-6)\n",
    "            - VendorID: int (1 or 2)\n",
    "            - RatecodeID: int (typically 1)\n",
    "            - trip_distance: float (miles, estimated)\n",
    "            - payment_type: int (1=credit, 2=cash, etc.)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Required features (NO POST-TRIP INFORMATION)\n",
    "        required_features = [\n",
    "            'tpep_pickup_datetime',\n",
    "            'pickup_longitude', 'pickup_latitude',\n",
    "            'dropoff_longitude', 'dropoff_latitude',\n",
    "            'passenger_count', 'VendorID', 'RatecodeID',\n",
    "            'trip_distance', 'payment_type'\n",
    "        ]\n",
    "        \n",
    "        # Check all required features are present\n",
    "        missing = [f for f in required_features if f not in trip_data]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required features: {missing}\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame([trip_data])\n",
    "        \n",
    "        # Preprocess and predict\n",
    "        X_processed = self.preprocessor.transform(df)\n",
    "        prediction = self.model.predict(X_processed)[0]\n",
    "        \n",
    "        return {\n",
    "            'predicted_duration_minutes': round(prediction, 1),\n",
    "            'confidence_interval': f\"{max(0, prediction-5):.1f} - {prediction+5:.1f} minutes\",\n",
    "            'features_used': len(self.preprocessor.named_steps['feature_engineer'].get_feature_names()),\n",
    "            'data_leakage_prevention': 'YES - only uses pickup-time information'\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize predictor\n",
    "    predictor = NYCTaxiPredictor('best_model.pkl', 'preprocessor.pkl')\n",
    "    \n",
    "    # Example trip (ALL information available at pickup)\n",
    "    example_trip = {\n",
    "        'tpep_pickup_datetime': '2016-01-15 17:30:00',\n",
    "        'pickup_longitude': -73.9855,\n",
    "        'pickup_latitude': 40.7580,\n",
    "        'dropoff_longitude': -73.9772,\n",
    "        'dropoff_latitude': 40.7829,\n",
    "        'passenger_count': 2,\n",
    "        'VendorID': 2,\n",
    "        'RatecodeID': 1,\n",
    "        'trip_distance': 2.5,  # Estimated route distance\n",
    "        'payment_type': 1\n",
    "    }\n",
    "    \n",
    "    result = predictor.predict(example_trip)\n",
    "    print(f\"\\nüöñ NYC Taxi Trip Duration Prediction (No Data Leakage)\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Predicted duration: {result['predicted_duration_minutes']} minutes\")\n",
    "    print(f\"95% confidence: {result['confidence_interval']}\")\n",
    "    print(f\"Features used: {result['features_used']}\")\n",
    "    print(f\"Data leakage prevention: {result['data_leakage_prevention']}\")\n",
    "'''\n",
    "\n",
    "script_path = os.path.join(model_save_dir, 'predict_trip_duration.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(prediction_script)\n",
    "print(f\"‚úÖ Prediction script saved: {script_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üíæ DEPLOYMENT PACKAGE READY (NO DATA LEAKAGE)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Location: {model_save_dir}\")\n",
    "print(f\"\\nüì¶ Contents:\")\n",
    "print(f\"  1. best_model.pkl - {best_model_name}\")\n",
    "print(f\"  2. preprocessor.pkl - Feature engineering pipeline\")\n",
    "print(f\"  3. model_card.json - Metadata (emphasizes no leakage)\")\n",
    "print(f\"  4. requirements.json - Dependencies\")\n",
    "print(f\"  5. predict_trip_duration.py - Production predictor\")\n",
    "print(f\"\\n‚úÖ KEY FEATURE: NO DATA LEAKAGE\")\n",
    "print(f\"   ‚Ä¢ Uses only pickup-time information\")\n",
    "print(f\"   ‚Ä¢ No post-trip features (fare, tip, etc.)\")\n",
    "print(f\"   ‚Ä¢ Realistic predictions at trip start\")\n",
    "print(f\"\\nüìä Expected performance: MAE ¬±{test_mae:.1f} min, R¬≤ {test_r2*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Summary: What We Fixed (No Data Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîí DATA LEAKAGE PREVENTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüö´ WHAT WE REMOVED (Data Leakage):\")\n",
    "print(\"  1. fare_amount, tip_amount, total_amount\")\n",
    "print(\"  2. extra, mta_tax, tolls_amount, improvement_surcharge\")\n",
    "print(\"  3. store_and_fwd_flag\")\n",
    "print(\"  4. Any feature calculated AFTER trip completion\")\n",
    "\n",
    "print(\"\\n‚úÖ WHAT WE KEPT (Available at Pickup):\")\n",
    "print(\"  1. Pickup datetime & location\")\n",
    "print(\"  2. Dropoff location (if destination entered)\")\n",
    "print(\"  3. Passenger count, Vendor ID\")\n",
    "print(\"  4. Rate code, Payment type\")\n",
    "print(\"  5. Trip distance (estimated route)\")\n",
    "\n",
    "print(\"\\nüîß KEY WORKFLOW CHANGES:\")\n",
    "print(\"  1. Calculate target FIRST (trip duration from timestamps)\")\n",
    "print(\"  2. Split data BEFORE feature engineering\")\n",
    "print(\"  3. Fit preprocessing pipeline on TRAINING only\")\n",
    "print(\"  4. Transform all datasets with SAME fitted pipeline\")\n",
    "\n",
    "print(\"\\nüéØ REALISTIC PREDICTION SCENARIO:\")\n",
    "print(\"  ‚Ä¢ Time: AT PICKUP\")\n",
    "print(\"  ‚Ä¢ Input: Only information known when trip starts\")\n",
    "print(\"  ‚Ä¢ Output: Predicted duration in minutes\")\n",
    "print(\"  ‚Ä¢ Confidence: Realistic error estimates (no leakage optimism)\")\n",
    "\n",
    "print(\"\\nüìä EXPECTED PERFORMANCE (Realistic):\")\n",
    "print(f\"  ‚Ä¢ R¬≤: {test_r2*100:.1f}% (lower but REALISTIC)\")\n",
    "print(f\"  ‚Ä¢ MAE: ¬±{test_mae:.1f} minutes\")\n",
    "print(f\"  ‚Ä¢ Within 10 min: {(np.abs(errors) <= 10).mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for REAL production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Final Celebration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"üéâ\" * 50)\n",
    "print(\"  NYC TAXI TRIP DURATION MODEL - NO DATA LEAKAGE!\")\n",
    "print(\"üéâ\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ ACCOMPLISHED:\")\n",
    "print(\"  1. Eliminated ALL data leakage\")\n",
    "print(\"  2. Realistic prediction at pickup time\")\n",
    "print(\"  3. Production-ready deployment package\")\n",
    "print(\"  4. MLflow experiment tracking\")\n",
    "\n",
    "print(f\"\\nüìä FINAL MODEL:\")\n",
    "print(f\"   ‚Ä¢ Model: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Test R¬≤: {test_r2:.4f} ({test_r2*100:.1f}% variance explained)\")\n",
    "print(f\"   ‚Ä¢ Test MAE: {test_mae:.2f} minutes\")\n",
    "print(f\"   ‚Ä¢ Avg trip: {y_test.mean():.1f} minutes\")\n",
    "\n",
    "print(f\"\\nüíæ Deployment package: {model_save_dir}\")\n",
    "print(f\"üìà MLflow experiments: mlflow ui --backend-store-uri {config.EXPERIMENT_DIR}\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for REAL production use!\")\n",
    "print(\"‚ú® Congratulations on building a REALISTIC, production-ready model! ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops-zoomcamp)",
   "language": "python",
   "name": "mlops-zoomcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
